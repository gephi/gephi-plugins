/*
 * Copyright (c) 2011, INRIA
 * All rights reserved.
 */
package fr.inria.edelweiss.semantic.importer;

import fr.inria.edelweiss.semantic.LayoutExamplePostProcessor;
import fr.inria.edelweiss.semantic.PluginProperties;
import fr.inria.edelweiss.semantic.analyzer.RdfAnalyzer;
import fr.inria.edelweiss.sparql.GephiUtils;
import fr.inria.edelweiss.sparql.SparqlDriver;
import java.util.Properties;
import java.util.concurrent.Semaphore;
import java.util.logging.Level;
import java.util.logging.Logger;
import org.gephi.graph.api.Graph;
import org.gephi.graph.api.GraphController;
import org.gephi.graph.api.GraphModel;
import org.gephi.project.api.ProjectController;
import org.gephi.project.api.Workspace;
import org.gephi.utils.longtask.api.LongTaskErrorHandler;
import org.gephi.utils.longtask.api.LongTaskExecutor;
import org.gephi.utils.longtask.api.LongTaskListener;
import org.gephi.utils.longtask.spi.LongTask;
import org.openide.util.Lookup;
import org.gephi.graph.api.Table;


/**
 * Responsible for making the request and building the resulting graph
 * asynchronously. The purpose of this class is to: <ol> <li>use a SPARQL driver
 * to obtain a RDF/XML result;</li> <li>populate the graph by extracting in the
 * result the nodes and the edges.</li> </ol>
 *
 * @author Erwan Demairy <Erwan.Demairy@inria.fr>
 */
public class SemanticWebImportParser implements LongTaskListener {

	private static final Logger logger = Logger.getLogger(SemanticWebImportParser.class.getName());
	private ProjectController pc;
	private Graph rdfGraph;
	private RequestParameters parameters;
	private RdfAnalyzer analyzer;
	private SparqlDriver driver;
	private final Semaphore waitEndPopulate = new Semaphore(0);
	private LongTaskListener listener; // listener waiting for the end of the population task.
	LongTaskExecutor executor = new LongTaskExecutor(true);

	static public class RequestParameters {

		private String rdfRequest;

		public RequestParameters(String rdfRequest) {
			setRdfRequest(rdfRequest);
		}

		public String getRdfRequest() {
			return this.rdfRequest;
		}

		public final void setRdfRequest(String nodeRdfRequest) {
			this.rdfRequest = nodeRdfRequest;
		}
	}

	public SemanticWebImportParser() {
		this(new RequestParameters(""));
	}

	public SemanticWebImportParser(RequestParameters requests) {
		this.parameters = requests;
	}

	/**
	 * Asynchronous operation that fills the current workspace.
	 *
	 * @sa waitEndpopulateRDFGraph
	 *
	 * @param driverUsed SPARQL request driver.
	 * @param properties Parameters to use.
	 */
	public final void populateRDFGraph(SparqlDriver driverUsed, Properties properties, LongTaskListener listener) {
		this.listener = listener;
		boolean resetWorkspace = Boolean.parseBoolean(properties.getProperty(PluginProperties.RESET_WORKSPACE.getValue(), "false"));
		boolean postProcessing = Boolean.parseBoolean(properties.getProperty(PluginProperties.POST_PROCESSING.getValue(), "false"));
		String saveResultName = properties.getProperty(PluginProperties.SAVE_SPARQL_RESULT.getValue(), "");
		int fynLevel = Integer.parseInt(properties.getProperty(PluginProperties.FYN_LEVEL.getValue(), "0"));

		logger.log(Level.INFO, "resetWorkspace = {0}", resetWorkspace);
		logger.log(Level.INFO, "postProcessing = {0}", postProcessing);

		this.driver = driverUsed;
		this.driver.setPluginProperties(properties);

		GephiUtils.createProjectIfEmpty();
		GephiUtils.createWorkspaceIfEmpty();

		pc = Lookup.getDefault().lookup(ProjectController.class);
		Workspace dataWorkspace = pc.getCurrentWorkspace();
		GraphModel model = getCurrentGraphModel(dataWorkspace);
		// @TODO how to reset the graph
//        if (resetWorkspace) {
//            model.getNodeTable().clear();
//        }
		Table nodeTable = model.getNodeTable();
		if (!nodeTable.hasColumn("namespace")) {
			nodeTable.addColumn("namespace", String.class);
		}

		analyzer = initAnalyzer(model, parameters.getRdfRequest(), fynLevel);
		analyzer.setSaveResult(saveResultName);
		if (postProcessing) {
			LayoutExamplePostProcessor postProcessor = new LayoutExamplePostProcessor();
			analyzer.setPostProcessing(postProcessor);
		}
		executor.setLongTaskListener(this);
		executor.execute(analyzer, analyzer, "Importing Semantic Graph", new LongTaskErrorHandler() {
			@Override
			public void fatalError(Throwable thrwbl) {
				thrwbl.printStackTrace();
			}
		});

	}

	@Override
	public void taskFinished(LongTask lt) {
		if (waitEndPopulate.availablePermits() < 1) {
			waitEndPopulate.release();
		}
		listener.taskFinished(lt);
	}

	public final void waitEndpopulateRDFGraph() throws InterruptedException {
		waitEndPopulate.acquire();
	}

	private RdfAnalyzer initAnalyzer(GraphModel model, String rdfRequest, int fynLevel) {
		driver.init();
		RdfAnalyzer localAnalyzer = new RdfAnalyzer(model, rdfRequest, fynLevel);
		localAnalyzer.setSparqlEngine(driver);
		return localAnalyzer;
	}

	public RequestParameters getParameters() {
		return parameters;
	}

	public void setParameters(RequestParameters newParameters) {
		this.parameters = newParameters;
	}

	private GraphModel getCurrentGraphModel(final Workspace currentWorkspace) {
		final GraphController currentGraphController = Lookup.getDefault().lookup(GraphController.class);
		return currentGraphController.getGraphModel(currentWorkspace);
	}

	/**
	 * @return the rdfGraph
	 */
	public Graph getRdfGraph() {
		return this.rdfGraph;
	}

	public String getLastRdfResult() {
		return analyzer.getSparqlRequestResult();
	}
}
